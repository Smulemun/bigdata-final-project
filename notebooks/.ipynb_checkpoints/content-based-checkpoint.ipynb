{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebfd49a-26b4-4040-889a-889f8436dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6339c12a-2873-4b0b-a05b-0549fb719515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4142\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>16 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f46ea041940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = 16\n",
    "nworkers = 3\n",
    "cores = 1\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .config('spark.executor.instances', nworkers)\\\n",
    "        .config(\"spark.executor.cores\", cores)\\\n",
    "        .config(\"spark.executor.cpus\", cores)\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "        # \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c467560-1c0b-4c51-a25a-3f1ba862ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext # !!!!!!\n",
    "sc.addPyFile('Net.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963d0b24-2ec1-4dfe-9dca-e10f73caa0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+-------------------+----------+-------------------+--------------------+----------+-------+---------+--------------------+-----------+\n",
      "|event_id|         event_time|product_id|        category_id|       category_code|     brand|  price|  user_id|        user_session|event_types|\n",
      "+--------+-------------------+----------+-------------------+--------------------+----------+-------+---------+--------------------+-----------+\n",
      "|42441406|2019-10-31 22:58:52|   4804409|2053013554658804075|electronics.audio...|     elari|  61.65|512831449|528d70c6-44ef-44e...|       cart|\n",
      "|42433956|2019-10-31 22:08:14|   5100570|2053013553341792533|  electronics.clocks|     apple|  447.6|520814382|06ec3176-a5c8-427...|       cart|\n",
      "|42412263|2019-10-31 20:37:34|   1005116|2053013555631882655|electronics.smart...|     apple|1013.86|515926715|f5453671-cfd2-4f7...|       cart|\n",
      "|42412081|2019-10-31 20:37:05|   1004888|2053013555631882655|electronics.smart...|   samsung| 224.46|562130094|1bf38b1f-a5d0-4f3...|       cart|\n",
      "|42407125|2019-10-31 20:24:47|  12702930|2053013553559896355|                    |  cordiant|  35.78|516726231|0ff774f1-cd3c-48b...|       cart|\n",
      "|42404853|2019-10-31 20:19:35|   1004902|2053013555631882655|electronics.smart...|      oppo| 591.91|566247089|ad89e035-0955-47f...|       cart|\n",
      "|42385729|2019-10-31 19:37:33|   4804056|2053013554658804075|electronics.audio...|     apple| 160.57|547205373|2636fa99-a739-43b...|       cart|\n",
      "|42382592|2019-10-31 19:32:41|   1004766|2053013555631882655|electronics.smart...|   samsung| 242.72|565511460|d59c2a9b-e6d1-418...|       cart|\n",
      "|42382215|2019-10-31 19:32:08|   1005014|2053013555631882655|electronics.smart...|   samsung| 503.09|515187084|40c8a253-bbde-4b1...|       cart|\n",
      "|42371400|2019-10-31 19:16:35|   7002254|2053013560346280633|       kids.carriage| wingoffly| 115.13|554229861|9206fff8-60c2-43c...|       cart|\n",
      "|42344365|2019-10-31 18:46:30|   1004767|2053013555631882655|electronics.smart...|   samsung| 242.64|566180105|767a219c-fe4b-49a...|       cart|\n",
      "|42343050|2019-10-31 18:45:10|   1005159|2053013555631882655|electronics.smart...|    xiaomi|  212.1|514609456|08c50250-a614-4d2...|       cart|\n",
      "|42341787|2019-10-31 18:43:53|   4803977|2053013554658804075|electronics.audio...|   samsung| 107.45|512934039|f0230a92-77e7-444...|       cart|\n",
      "|42341673|2019-10-31 18:43:46|   1004238|2053013555631882655|electronics.smart...|     apple|1243.78|513849031|b7b32e1c-4afe-469...|       cart|\n",
      "|42338936|2019-10-31 18:40:51|   1002544|2053013555631882655|electronics.smart...|     apple| 458.28|565491098|64314c34-5452-42f...|       cart|\n",
      "|42330148|2019-10-31 18:32:13|   1002544|2053013555631882655|electronics.smart...|     apple| 458.28|530619133|1b211a76-83c0-4ac...|       cart|\n",
      "|42324454|2019-10-31 18:26:54|   1004767|2053013555631882655|electronics.smart...|   samsung| 242.64|566214562|2569f2e4-b376-45c...|       cart|\n",
      "|42309647|2019-10-31 18:14:05|   1004781|2053013555631882655|electronics.smart...|    huawei| 254.73|549684152|83925762-aa6f-4d1...|       cart|\n",
      "|42287761|2019-10-31 17:56:22|   7900883|2053013556487520725|furniture.kitchen...|peg-perego| 280.57|559052401|9fc56e81-0b4a-40e...|       cart|\n",
      "|42253644|2019-10-31 17:31:25|   1801690|2053013554415534427|electronics.video.tv|   samsung| 369.45|515921270|125a7248-8c7a-46a...|       cart|\n",
      "+--------+-------------------+----------+-------------------+--------------------+----------+-------+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE team16_projectdb\").show()\n",
    "spark.sql(\"SELECT * FROM ecom_part_buck\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cbe2d2e-7df0-4f3f-b8c5-6552956b9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: integer (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_types: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.format(\"avro\").table('team16_projectdb.ecom_part_buck')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7683c2d-b822-4b4f-aeca-209d51ef979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicTransformer(Transformer):\n",
    "    def __init__(self, input_col):\n",
    "        super(CyclicTransformer, self).__init__()\n",
    "        self.input_col = input_col\n",
    "\n",
    "    def _transform(self, df):\n",
    "        extract_year = F.udf(lambda x: x.year)\n",
    "        extract_month = F.udf(lambda x: x.month)\n",
    "        extract_day = F.udf(lambda x: x.day)\n",
    "        extract_hour = F.udf(lambda x: x.hour)\n",
    "        extract_minute = F.udf(lambda x: x.minute)\n",
    "        extract_second = F.udf(lambda x: x.second)\n",
    "\n",
    "        return df.withColumn('year', extract_year(self.input_col))\\\n",
    "                 .withColumn('month', extract_month(self.input_col))\\\n",
    "                 .withColumn('day', extract_day(self.input_col))\\\n",
    "                 .withColumn('hour', extract_hour(self.input_col))\\\n",
    "                 .withColumn('minute', extract_minute(self.input_col))\\\n",
    "                 .withColumn('second', extract_second(self.input_col))\\\n",
    "                 .withColumn('month_sin', F.sin(F.col('month') * 2 * math.pi / 12))\\\n",
    "                 .withColumn('month_cos', F.cos(F.col('month') * 2 * math.pi / 12))\\\n",
    "                 .withColumn('day_sin', F.sin(F.col('day') * 2 * math.pi / 31))\\\n",
    "                 .withColumn('day_cos', F.cos(F.col('day') * 2 * math.pi / 31))\\\n",
    "                 .withColumn('hour_sin', F.sin(F.col('hour') * 2 * math.pi / 24))\\\n",
    "                 .withColumn('hour_cos', F.cos(F.col('hour') * 2 * math.pi / 24))\\\n",
    "                 .withColumn('minute_sin', F.sin(F.col('minute') * 2 * math.pi / 60))\\\n",
    "                 .withColumn('minute_cos', F.cos(F.col('minute') * 2 * math.pi / 60))\\\n",
    "                 .withColumn('second_sin', F.sin(F.col('second') * 2 * math.pi / 60))\\\n",
    "                 .withColumn('second_cos', F.cos(F.col('second') * 2 * math.pi / 60))\\\n",
    "                 .drop('month').drop('day')\\\n",
    "                 .drop('hour').drop('minute').drop('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9ac9bf-780b-4fb1-bb07-b851711f90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+-------------------+--------------------+-----+-----+---------+--------------------+-----------+----+-------------------+------------------+--------------------+-------+-------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|event_id|         event_time|product_id|        category_id|       category_code|brand|price|  user_id|        user_session|event_types|year|          month_sin|         month_cos|             day_sin|day_cos|           hour_sin|          hour_cos|          minute_sin|        minute_cos|        second_sin|        second_cos|\n",
      "+--------+-------------------+----------+-------------------+--------------------+-----+-----+---------+--------------------+-----------+----+-------------------+------------------+--------------------+-------+-------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|42441406|2019-10-31 22:58:52|   4804409|2053013554658804075|electronics.audio...|elari|61.65|512831449|528d70c6-44ef-44e...|       cart|2019|-0.8660254037844386|0.5000000000000001|-2.44929359829470...|    1.0|-0.5000000000000004|0.8660254037844384|-0.20791169081775898|0.9781476007338057|-0.743144825477394|0.6691306063588585|\n",
      "+--------+-------------------+----------+-------------------+--------------------+-----+-----+---------+--------------------+-----------+----+-------------------+------------------+--------------------+-------+-------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cyclic_trans = CyclicTransformer('event_time')\n",
    "data = cyclic_trans.transform(data)\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a9d48c-65d9-4572-8e24-3094f2e67b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.na.drop(subset=data.columns)\n",
    "data = data.filter(data.brand != '')\n",
    "data = data.filter(data.category_code != '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af78813-7816-4944-ba95-db938f282641",
   "metadata": {},
   "source": [
    "data.groupBy(\"category_code\").count().withColumnRenamed(\"count\", \"num\").orderBy(F.col(\"num\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677dc8d3-03e0-40b9-be4d-b7c8091789cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: integer (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month_sin: double (nullable = true)\n",
      " |-- month_cos: double (nullable = true)\n",
      " |-- day_sin: double (nullable = true)\n",
      " |-- day_cos: double (nullable = true)\n",
      " |-- hour_sin: double (nullable = true)\n",
      " |-- hour_cos: double (nullable = true)\n",
      " |-- minute_sin: double (nullable = true)\n",
      " |-- minute_cos: double (nullable = true)\n",
      " |-- second_sin: double (nullable = true)\n",
      " |-- second_cos: double (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_type_to_rating = F.udf(lambda x: 1 if x == 'purchase' else 0 if x == 'cart' else -1, IntegerType())\n",
    "data = data.withColumn('rating', event_type_to_rating('event_types')).drop('event_types')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb0bc08-11fd-4174-a40e-b226babc4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brand preprocessing: take only popular brands (> 10000 interactions)\n",
    "\n",
    "brand_counts = data.groupBy(\"brand\").count()\n",
    "rare_brands = brand_counts.filter(F.col(\"count\") < 10000).select(\"brand\").rdd.flatMap(lambda x: x).collect()\n",
    "data = data.withColumn(\"brand\", F.when(F.col(\"brand\").isin(rare_brands), \"other\").otherwise(F.col(\"brand\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d016c8-9c45-4d29-b1e6-4c7f89fe724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of brand\n",
    "\n",
    "# indexer = StringIndexer(inputCol='brand', outputCol=\"brand_indexed\")\n",
    "# oh_encoder = OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"brand_encoded\")\n",
    "\n",
    "# brand_pipeline = Pipeline(stages=[indexer, oh_encoder]).fit(data)\n",
    "# data = brand_pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb807e0-b0ff-45ee-a46a-a18fc4e63cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fc414-256e-421d-af34-2ec898e526cd",
   "metadata": {},
   "source": [
    "# Content-based RecSys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2fa938a-9e73-4b0b-8c38-ffbacc05dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['user_id']\n",
    "item_features = ['product_id', 'category_code', 'brand', 'price']\n",
    "session_features = ['year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
    "target = 'rating'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2294ea-8a53-4476-899a-8ace38c3e33a",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae766581-3fca-401d-a602-36e1e03d0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+-------+-------+\n",
      "|  user_id|product_id|       category_code|  brand|  price|\n",
      "+---------+----------+--------------------+-------+-------+\n",
      "|337535108|  14100411|electronics.audio...|  other| 148.88|\n",
      "|337535108|   1307285|  computers.notebook|     hp|1407.76|\n",
      "|337535108|   1307285|  computers.notebook|     hp|1407.76|\n",
      "|337535108|   1004529|electronics.smart...|samsung| 396.15|\n",
      "|337535108|   1004529|electronics.smart...|samsung| 396.15|\n",
      "|337535108|   1307472|  computers.notebook|  other|1363.19|\n",
      "|404666934|   1306651|  computers.notebook|   acer| 488.82|\n",
      "|404666934|   1307144|  computers.notebook|     hp|  599.5|\n",
      "|404666934|   1307435|  computers.notebook| lenovo| 450.44|\n",
      "|404666934|   1307293|  computers.notebook|  other| 868.93|\n",
      "+---------+----------+--------------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leave users only with more than 5 events(purchase, cart, view), so that we can split to train/test for each user by event_time with 0.2 ratio\n",
    "user_interaction_counts = data.groupBy('user_id').count()\n",
    "active_users = user_interaction_counts.filter(F.col('count') > 5)\n",
    "data_filtered = data.join(active_users, 'user_id', 'inner')\n",
    "data_filtered.select(user_features + item_features).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fabc26-cb9a-4962-b497-992289848d24",
   "metadata": {},
   "source": [
    "# Scaling Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf180323-7085-4db0-8738-acdc5f575328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data_filtered.withColumn('year', F.col('year').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "189d2a91-8aaa-4968-a319-940f884d02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaling numerical columns\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "columns_to_scale = ['year', 'price']\n",
    "\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(data_filtered)\n",
    "scaled_data = scalerModel.transform(data_filtered)\n",
    "\n",
    "scaled_data = scaled_data.drop(*['year', 'price', 'year_vec', 'price_vec'])\n",
    "\n",
    "udf_extract_double = udf(lambda vector: vector.tolist()[0], FloatType())\n",
    "scaled_data = scaled_data.withColumn(\"year\", udf_extract_double(\"year_scaled\")).withColumn(\"price\", udf_extract_double(\"price_scaled\"))\n",
    "scaled_data = scaled_data.drop(*['year_scaled','price_scaled','count', 'category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fe8088-f455-4491-b37a-b6e77c319f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+-------+----------+\n",
      "|  user_id|product_id|       category_code|  brand|     price|\n",
      "+---------+----------+--------------------+-------+----------+\n",
      "|337535108|   1004529|electronics.smart...|samsung|0.15361089|\n",
      "+---------+----------+--------------------+-------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_data.select(user_features + item_features).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a50e89f-6e3a-461c-b63d-38a19f2f78f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|year|          month_sin|         month_cos|            day_sin|           day_cos|           hour_sin|           hour_cos|        minute_sin|        minute_cos|        second_sin|        second_cos|\n",
      "+----+-------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "| 0.5|-0.8660254037844386|0.5000000000000001|0.20129852008866006|0.9795299412524945|-0.4999999999999997|-0.8660254037844388|0.3090169943749474|0.9510565162951535|0.6691306063588582|0.7431448254773942|\n",
      "+----+-------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_data.select(session_features).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424c231-2129-4384-82f2-bb44f1812191",
   "metadata": {},
   "source": [
    "# Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f50e4b-db44-40e1-a588-a24afa1cd81a",
   "metadata": {},
   "source": [
    "### User_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27d5bb5e-41c9-4e51-903e-f3ad2ad2d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = scaled_data.select('user_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "user_id_mapping = {_id: idx for idx, _id in enumerate(unique_user_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90855805-2440-4854-ae61-9802357461c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_mapper = F.udf(lambda x: user_id_mapping[x], IntegerType())\n",
    "mapped_data = scaled_data.withColumn('user_id', user_id_mapper('user_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fec64-61d5-4fa2-b911-af11be7634e0",
   "metadata": {},
   "source": [
    "### Product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21e3d9fc-889b-48c5-81cf-e5e67a38c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_product_ids = mapped_data.select('product_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "product_id_mapping = {_id: idx for idx, _id in enumerate(unique_product_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a52b0d1a-e0de-406a-a363-800fac19a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id_mapper = F.udf(lambda x: product_id_mapping[x], IntegerType())\n",
    "mapped_data = mapped_data.withColumn('product_id', product_id_mapper('product_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65807d1-fc4b-4e16-a8a5-a43c47ce30ea",
   "metadata": {},
   "source": [
    "### Category_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24a0b6de-8284-45b3-a977-a2c87dbbbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Word2Vec, RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol='category_code', outputCol='tokenized_category', pattern=\"\\.\")\n",
    "word2Vec = Word2Vec(vectorSize=16, seed=42, minCount=1, inputCol='tokenized_category', outputCol='category_embedding')\n",
    "embedding_pipeline = Pipeline(stages=[tokenizer, word2Vec]).fit(mapped_data)\n",
    "mapped_data = embedding_pipeline.transform(mapped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67d0c7f0-28e6-44e0-aeaf-96aa9295cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = mapped_data.drop('category_code').withColumnRenamed(\"category_embedding\", \"category_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8595db-7537-4849-aaa5-421621ea8707",
   "metadata": {},
   "source": [
    "### Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55b74da1-8a58-458c-82e4-556e7e5c303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_brand_ids = mapped_data.select('brand').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "brand_id_mapping = {_id: idx for idx, _id in enumerate(unique_brand_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a70df61-5a35-4bf8-b36b-6310a7e80bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_id_mapper = F.udf(lambda x: brand_id_mapping[x], IntegerType())\n",
    "mapped_data = mapped_data.withColumn('brand', brand_id_mapper('brand'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "284d97a9-c50d-4ad6-9c1e-6105a988fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+-----+----------+\n",
      "|user_id|product_id|       category_code|brand|     price|\n",
      "+-------+----------+--------------------+-----+----------+\n",
      "|      0|     27733|[0.26988664269447...|   16|0.15361089|\n",
      "+-------+----------+--------------------+-----+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapped_data.select(user_features + item_features).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaaeb3f-a3e8-4988-9c4e-4db3830458b9",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "179e757b-09eb-4e20-b003-6ea1411ba28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('user_id').orderBy('event_time')\n",
    "\n",
    "df_with_row_number = mapped_data.withColumn('row_number', F.row_number().over(window_spec))\n",
    "\n",
    "user_count_window = Window.partitionBy('user_id')\n",
    "total_user_count = F.count('user_id').over(user_count_window)\n",
    "\n",
    "# Calculate the 80% threshold for each user group\n",
    "train_test_ratio = 0.8\n",
    "split_threshold = (total_user_count * train_test_ratio).cast('int')\n",
    "\n",
    "# Assign a 'train' or 'test' label based on the row number and the split threshold\n",
    "df_labeled = df_with_row_number.withColumn('split', F.when(F.col('row_number') <= split_threshold, 'train').otherwise('test'))\n",
    "\n",
    "# Split the DataFrame into train and test sets based on the label\n",
    "train_df = df_labeled.filter(F.col('split') == 'train').drop('row_number', 'split')\n",
    "test_df = df_labeled.filter(F.col('split') == 'test').drop('row_number', 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f2c43f5-37ca-4828-9549-bd2e27d050c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnt, test_cnt = train_df.count(), test_df.count()\n",
    "\n",
    "assert train_df.select('user_id').distinct().count() == test_df.select('user_id').distinct().count()\n",
    "assert train_test_ratio - 0.05 <= (train_cnt / (train_cnt + test_cnt)) <= train_test_ratio + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0154541-1bb3-414e-b3d5-daff9ed0c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_users = mapped_data.select('user_id').distinct().count()\n",
    "N_products = mapped_data.select('product_id').distinct().count()\n",
    "N_brands = mapped_data.select('brand').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc762f88-2d76-4624-b1d0-074f1b2210d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+----------+-----+--------------------+-------------------+------------------+------------------+------------------+--------+--------------------+------------------+------------------+-------------------+------------------+------+----+---------+--------------------+--------------------+\n",
      "|user_id|event_id|         event_time|product_id|brand|        user_session|          month_sin|         month_cos|           day_sin|           day_cos|hour_sin|            hour_cos|        minute_sin|        minute_cos|         second_sin|        second_cos|rating|year|    price|  tokenized_category|       category_code|\n",
      "+-------+--------+-------------------+----------+-----+--------------------+-------------------+------------------+------------------+------------------+--------+--------------------+------------------+------------------+-------------------+------------------+------+----+---------+--------------------+--------------------+\n",
      "|      1| 3426810|2019-10-03 18:10:01|     16494|   26|d95b3c6e-2490-492...|-0.8660254037844386|0.5000000000000001|0.5712682150947923|0.8207634412072763|    -1.0|-1.83697019872102...|0.8660254037844386|0.5000000000000001|0.10452846326765346|0.9945218953682733|    -1| 0.5|0.2326373|[computers, noteb...|[-1.3485463559627...|\n",
      "+-------+--------+-------------------+----------+-----+--------------------+-------------------+------------------+------------------+------------------+--------+--------------------+------------------+------------------+-------------------+------------------+------+----+---------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38a63922-a164-447c-9156-67a0c6856ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'product_id',\n",
       " 'category_code',\n",
       " 'brand',\n",
       " 'price',\n",
       " 'year',\n",
       " 'month_sin',\n",
       " 'month_cos',\n",
       " 'day_sin',\n",
       " 'day_cos',\n",
       " 'hour_sin',\n",
       " 'hour_cos',\n",
       " 'minute_sin',\n",
       " 'minute_cos',\n",
       " 'second_sin',\n",
       " 'second_cos']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features + item_features + session_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5adbeb0c-825d-4243-9076-afe92a911cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=user_features + item_features + session_features, outputCol='features')\n",
    "\n",
    "train_df = vector_assembler.transform(train_df)\n",
    "test_df = vector_assembler.transform(test_df)\n",
    "# train_df = train_df.select(['features', target])\n",
    "# test_df = test_df.select(['features', target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96c4496f-7954-4e27-aa75-5c79ebe04c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 16494.0, -1.3485, -0.5481, 0.6711, -0.0213, 0.4279, 0.6109, 0.5394, -0.5986, -0.4995, -1.1732, 0.0095, 0.0903, 0.4572, -1.5935, 0.7368, -0.1227, 26.0, 0.2326, 0.5, -0.866, 0.5, 0.5713, 0.8208, -1.0, -0.0, 0.866, 0.5, 0.1045, 0.9945]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select('features').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7139a28f-7f40-4ede-a27b-78c0c71d71b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1, event_id=3426810, event_time=datetime.datetime(2019, 10, 3, 18, 10, 1), product_id=16494, brand=26, user_session='d95b3c6e-2490-4924-a851-a22f3cf78ec7', month_sin=-0.8660254037844386, month_cos=0.5000000000000001, day_sin=0.5712682150947923, day_cos=0.8207634412072763, hour_sin=-1.0, hour_cos=-1.8369701987210297e-16, minute_sin=0.8660254037844386, minute_cos=0.5000000000000001, second_sin=0.10452846326765346, second_cos=0.9945218953682733, rating=-1, year=0.5, price=0.23263730108737946, tokenized_category=['computers', 'notebook'], category_code=DenseVector([-1.3485, -0.5481, 0.6711, -0.0213, 0.4279, 0.6109, 0.5394, -0.5986, -0.4995, -1.1732, 0.0095, 0.0903, 0.4572, -1.5935, 0.7368, -0.1227]), features=DenseVector([1.0, 16494.0, -1.3485, -0.5481, 0.6711, -0.0213, 0.4279, 0.6109, 0.5394, -0.5986, -0.4995, -1.1732, 0.0095, 0.0903, 0.4572, -1.5935, 0.7368, -0.1227, 26.0, 0.2326, 0.5, -0.866, 0.5, 0.5713, 0.8208, -1.0, -0.0, 0.866, 0.5, 0.1045, 0.9945]))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f573626-adc6-40d2-bb0f-445299e228e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "array_to_vector = F.udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "def load_data(split, spark, path='project/data', feature='features'):\n",
    "    df = spark.read.json(os.path.join(path, split))\n",
    "    df = df.withColumn(feature, F.col(feature).getField(\"values\"))\\\n",
    "           .withColumn(feature, array_to_vector(F.col(feature)))\n",
    "    print(f'Loaded {os.path.join(path, split)}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee348947-1cdf-4ca6-aa41-64849563deeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded project/data/train\n",
      "Loaded project/data/test\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data(path='project/data', spark=spark, split='train', feature='category_code')\n",
    "test_df = load_data(path='project/data', spark=spark, split='test', feature='category_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc828a8-4799-42e4-b59f-12b73ed41fad",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "049728e2-8930-4e64-8e1e-b0104229b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to .sh scripts\n",
    "# pip install torch\n",
    "# pip install sparktorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49c6af7-efb2-4d61-9f28-765e92ea72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Net import Content_based_filtering\n",
    "# sample = list(train_df.take(1)[0]['features'])\n",
    "# model = Content_based_filtering(n_brands=N_brands, n_items=N_products, n_users=N_users)\n",
    "# model(torch.tensor(sample, dtype=torch.float32).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745ddb6e-c56b-46b1-be9b-e952d229e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ded669-8298-45d8-a0d3-eb6a2c09a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparktorch import serialize_torch_obj, SparkTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = Content_based_filtering(\n",
    "    n_brands=34,\n",
    "    n_items=39699,\n",
    "    n_users=97917,\n",
    "    brand_dim=32,\n",
    "    dim=16,\n",
    ")\n",
    "print('created model')\n",
    "torch_obj = serialize_torch_obj(\n",
    "    model=model,\n",
    "    criterion=nn.L1Loss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001,\n",
    ")\n",
    "print('created torch obj')\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='features',\n",
    "    labelCol=target,\n",
    "    predictionCol='prediction',\n",
    "    torchObj=torch_obj,\n",
    "    iters=1,\n",
    "    miniBatch=16,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "597bafc6-57db-4c02-9082-df98b8f9a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_model = SparkTorch(\n",
    "    inputCol='features',\n",
    "    labelCol=target,\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=1,\n",
    "    miniBatch=16,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5499b7af-be0c-4283-9cb0-6fd22d66ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.repartition(cores)\n",
    "# test_df = test_df.repartition(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b324786-3c39-4abf-8f15-7b2a3ee010fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.scheduler.BarrierJobRunWithDynamicAllocationException: [SPARK-24942]: Barrier execution mode does not support dynamic resource allocation for now. You can disable dynamic resource allocation by setting Spark conf \"spark.dynamicAllocation.enabled\" to \"false\".\n\tat org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithDynamicAllocation(DAGScheduler.scala:500)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:588)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2588)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7468b7aaba13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspark_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sparktorch/torch_distributed.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sparktorch/distributed.py\u001b[0m in \u001b[0;36mtrain_distributed\u001b[0;34m(rdd, torch_obj, iters, partition_shuffles, verbose, mini_batch, validation_pct, world_size, device, early_stop_patience)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;31m# Run model with barrier execution mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             state_dict = mapPartitionsWithIndex(\n\u001b[0;32m--> 254\u001b[0;31m                 rdd, lambda i, x: handle_model(\n\u001b[0m\u001b[1;32m    255\u001b[0m                     \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \"\"\"\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.scheduler.BarrierJobRunWithDynamicAllocationException: [SPARK-24942]: Barrier execution mode does not support dynamic resource allocation for now. You can disable dynamic resource allocation by setting Spark conf \"spark.dynamicAllocation.enabled\" to \"false\".\n\tat org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithDynamicAllocation(DAGScheduler.scala:500)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:588)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2588)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "p = Pipeline(stages=[spark_model]).fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb9a2962-f5c5-4ca2-b46e-188eafc62fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 50438)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 540, in send_command\n",
      "    \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "java does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-677c1a5eda6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/test_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sparktorch/pipeline_util.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;34m\"\"\"Returns an MLWriter instance for this ML instance.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mJavaMLWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJavaMLWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sparktorch/pipeline_util.py\u001b[0m in \u001b[0;36m_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mpylist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mpylist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPysparkObjId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getPyObjId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add our id so PysparkPipelineWrapper can id us.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mjava_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mjava_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mjava_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpylist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1720\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1721\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1722\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: java does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "spark_model.write().overwrite().save('../models/test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d73c4-5c98-47bc-8ea0-91b3d5a67322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparktorch import PysparkPipelineWrapper\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "trained_model = PysparkPipelineWrapper.unwrap(PipelineModel.load('../models/test_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d44c8-b339-4f05-99ff-d2d9bbec07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.transform(test_df).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839206e-3696-4019-9b8c-473682c78ade",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aff356-4642-4583-93cb-b051aec1ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b1083-dc1e-46b3-ab92-b6ebb916b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6018d-9855-4f59-b4b7-02c5a6d7ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571bfa7b-0484-4f56-b3e6-11f63e02ace1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
