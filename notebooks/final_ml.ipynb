{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db01da1c-317f-471b-a1e2-bf61e585724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Word2Vec, Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql.functions import udf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99fc14f-0253-40e2-a6f1-add6334f502e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4142\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>16 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fea428357b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = 16\n",
    "nworkers = 1 # was 3 !!!!\n",
    "cores = 1\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .config('spark.executor.instances', nworkers)\\\n",
    "        .config(\"spark.executor.cores\", cores)\\\n",
    "        .config(\"spark.executor.cpus\", cores)\\\n",
    "        .config(\"spark.executor.memory\", \"1g\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ecbd71-0246-48d1-a5ef-9ddf6aff3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.addPyFile('Net.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b74b562-51a5-4c65-81cc-62db6d8b8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"avro\").table('team16_projectdb.ecom_part_buck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e53179-02fe-4d46-85d8-fcf5b6c6e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicTransformer(Transformer):\n",
    "    def __init__(self, input_col):\n",
    "        super(CyclicTransformer, self).__init__()\n",
    "        self.input_col = input_col\n",
    "\n",
    "    def _transform(self, df):\n",
    "        extract_year = F.udf(lambda x: x.year)\n",
    "        extract_month = F.udf(lambda x: x.month)\n",
    "        extract_day = F.udf(lambda x: x.day)\n",
    "        extract_hour = F.udf(lambda x: x.hour)\n",
    "        extract_minute = F.udf(lambda x: x.minute)\n",
    "        extract_second = F.udf(lambda x: x.second)\n",
    "\n",
    "        return df.withColumn('year', extract_year(self.input_col))\\\n",
    "                 .withColumn('month', extract_month(self.input_col))\\\n",
    "                 .withColumn('day', extract_day(self.input_col))\\\n",
    "                 .withColumn('hour', extract_hour(self.input_col))\\\n",
    "                 .withColumn('minute', extract_minute(self.input_col))\\\n",
    "                 .withColumn('second', extract_second(self.input_col))\\\n",
    "                 .withColumn('month_sin', F.sin(F.col('month') * 2 * math.pi / 12))\\\n",
    "                 .withColumn('month_cos', F.cos(F.col('month') * 2 * math.pi / 12))\\\n",
    "                 .withColumn('day_sin', F.sin(F.col('day') * 2 * math.pi / 31))\\\n",
    "                 .withColumn('day_cos', F.cos(F.col('day') * 2 * math.pi / 31))\\\n",
    "                 .withColumn('hour_sin', F.sin(F.col('hour') * 2 * math.pi / 24))\\\n",
    "                 .withColumn('hour_cos', F.cos(F.col('hour') * 2 * math.pi / 24))\\\n",
    "                 .withColumn('minute_sin', F.sin(F.col('minute') * 2 * math.pi / 60))\\\n",
    "                 .withColumn('minute_cos', F.cos(F.col('minute') * 2 * math.pi / 60))\\\n",
    "                 .withColumn('second_sin', F.sin(F.col('second') * 2 * math.pi / 60))\\\n",
    "                 .withColumn('second_cos', F.cos(F.col('second') * 2 * math.pi / 60))\\\n",
    "                 .drop('month').drop('day')\\\n",
    "                 .drop('hour').drop('minute').drop('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea04111-b7e8-45c1-817f-c281e874b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclic_trans = CyclicTransformer('event_time')\n",
    "data = cyclic_trans.transform(data)\n",
    "data = data.na.drop(subset=data.columns)\n",
    "data = data.filter(data.brand != '')\n",
    "data = data.filter(data.category_code != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21839c71-48cd-4ec9-93ce-71b6aefa19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type_to_rating = F.udf(lambda x: 1 if x == 'purchase' else 0 if x == 'cart' else -1, IntegerType())\n",
    "data = data.withColumn('rating', event_type_to_rating('event_types')).drop('event_types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdcc7ff-ff32-4d72-85ac-38c5bc54c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_counts = data.groupBy(\"brand\").count()\n",
    "rare_brands = brand_counts.filter(F.col(\"count\") < 10000).select(\"brand\").rdd.flatMap(lambda x: x).collect()\n",
    "data = data.withColumn(\"brand\", F.when(F.col(\"brand\").isin(rare_brands), \"other\").otherwise(F.col(\"brand\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf9e49d-e837-4532-b55d-cf1ab51fb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['user_id']\n",
    "item_features = ['product_id', 'category_code', 'brand', 'price']\n",
    "session_features = ['year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
    "target = 'rating'\n",
    "\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(data_filtered)\n",
    "scaled_data = scalerModel.transform(data_filtered)\n",
    "\n",
    "scaled_data = scaled_data.drop(*['year', 'price', 'year_vec', 'price_vec'])\n",
    "\n",
    "udf_extract_double = udf(lambda vector: vector.tolist()[0], FloatType())\n",
    "scaled_data = scaled_data.withColumn(\"year\", udf_extract_double(\"year_scaled\")).withColumn(\"price\", udf_extract_double(\"price_scaled\"))\n",
    "scaled_data = scaled_data.drop(*['year_scaled','price_scaled','count', 'category_id'])price']\n",
    "session_features = ['year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
    "target = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a1b157-f173-435b-a3c4-a1a49ee998b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interaction_counts = data.groupBy('user_id').count()\n",
    "active_users = user_interaction_counts.filter(F.col('count') > 5)\n",
    "data_filtered = data.join(active_users, 'user_id', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3807bd1-8a5e-4b72-829e-d79f9c393f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data_filtered.withColumn('year', F.col('year').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "556d253f-6ac8-4e43-998d-a3d90449bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = ['year', 'price']\n",
    "\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(data_filtered)\n",
    "scaled_data = scalerModel.transform(data_filtered)\n",
    "\n",
    "scaled_data = scaled_data.drop(*['year', 'price', 'year_vec', 'price_vec'])\n",
    "\n",
    "udf_extract_double = udf(lambda vector: vector.tolist()[0], FloatType())\n",
    "scaled_data = scaled_data.withColumn(\"year\", udf_extract_double(\"year_scaled\")).withColumn(\"price\", udf_extract_double(\"price_scaled\"))\n",
    "scaled_data = scaled_data.drop(*['year_scaled','price_scaled','count', 'category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92eb5986-7171-4bdf-a039-fef24a2eb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = scaled_data.select('user_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "user_id_mapping = {_id: idx for idx, _id in enumerate(unique_user_ids)}\n",
    "user_id_mapper = F.udf(lambda x: user_id_mapping[x], IntegerType())\n",
    "mapped_data = scaled_data.withColumn('user_id', user_id_mapper('user_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451dccca-3d22-4aa9-ae82-bc2a9b93204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_product_ids = mapped_data.select('product_id').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "product_id_mapping = {_id: idx for idx, _id in enumerate(unique_product_ids)}\n",
    "product_id_mapper = F.udf(lambda x: product_id_mapping[x], IntegerType())\n",
    "mapped_data = mapped_data.withColumn('product_id', product_id_mapper('product_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd7a7af-a257-4b86-b396-8044d2e06a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='category_code', outputCol='tokenized_category', pattern=\"\\.\")\n",
    "word2Vec = Word2Vec(vectorSize=16, seed=42, minCount=1, inputCol='tokenized_category', outputCol='category_embedding')\n",
    "embedding_pipeline = Pipeline(stages=[tokenizer, word2Vec]).fit(mapped_data)\n",
    "mapped_data = embedding_pipeline.transform(mapped_data)\n",
    "mapped_data = mapped_data.drop('category_code').withColumnRenamed(\"category_embedding\", \"category_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e8a9bf-e72a-428e-9cdf-a3f4163396cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_brand_ids = mapped_data.select('brand').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "brand_id_mapping = {_id: idx for idx, _id in enumerate(unique_brand_ids)}\n",
    "brand_id_mapper = F.udf(lambda x: brand_id_mapping[x], IntegerType())\n",
    "mapped_data = mapped_data.withColumn('brand', brand_id_mapper('brand'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85019ed1-91bb-4eb7-8c85-876933d44955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_users = mapped_data.select('user_id').distinct().count()\n",
    "# N_products = mapped_data.select('product_id').distinct().count()\n",
    "# N_brands = mapped_data.select('brand').distinct().count()\n",
    "N_users = 97917\n",
    "N_products = 39699\n",
    "N_brands = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "849bbe3d-e76b-41d5-a07d-8398b15ba87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97917, 39699, 34)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_users, N_products, N_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a8a158-b5d7-4b21-8ffa-2bc829a77061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('user_id').orderBy('event_time')\n",
    "\n",
    "df_with_row_number = mapped_data.withColumn('row_number', F.row_number().over(window_spec))\n",
    "\n",
    "user_count_window = Window.partitionBy('user_id')\n",
    "total_user_count = F.count('user_id').over(user_count_window)\n",
    "\n",
    "# Calculate the 80% threshold for each user group\n",
    "train_test_ratio = 0.8\n",
    "split_threshold = (total_user_count * train_test_ratio).cast('int')\n",
    "\n",
    "# Assign a 'train' or 'test' label based on the row number and the split threshold\n",
    "df_labeled = df_with_row_number.withColumn('split', F.when(F.col('row_number') <= split_threshold, 'train').otherwise('test'))\n",
    "\n",
    "# Split the DataFrame into train and test sets based on the label\n",
    "train_df = df_labeled.filter(F.col('split') == 'train').drop('row_number', 'split')\n",
    "test_df = df_labeled.filter(F.col('split') == 'test').drop('row_number', 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6f4203a-293d-41eb-ab86-bdec98a348e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.select(user_features + item_features + session_features + [target])\n",
    "test_df = test_df.select(user_features + item_features + session_features + [target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9ceb1c-175f-444d-b298-d68c3cf56e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "train_df.coalesce(1)\\\n",
    "        .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .format(\"json\")\\\n",
    "        .save(\"project/data/train\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")\n",
    "\n",
    "test_df.coalesce(1)\\\n",
    "       .write\\\n",
    "       .mode(\"overwrite\")\\\n",
    "      .format(\"json\")\\\n",
    "      .save(\"project/data/test\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36dfc5-edec-47f7-a975-643bf25bb8bc",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e324e87-ebe4-4bb6-bc06-1080fddc6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['user_id']\n",
    "item_features = ['product_id', 'category_code', 'brand', 'price']\n",
    "session_features = ['year', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
    "target = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6625bf2-7727-4d24-b1b0-83dc81c5fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.json('project/data/train') # !!!!!\n",
    "test_df = spark.read.json('project/data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9488aee4-fbca-430b-9902-7d44e71bd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\"category_code\", F.col(\"category_code\").getField(\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e138e1d-a6d6-4a08-877b-e66dca3bcadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "array_to_vector = F.udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "train_df = train_df.withColumn(\"category_code\", array_to_vector(F.col(\"category_code\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9910446-c460-410d-884d-c0dacb806eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+---------+----------+------+-------------------+------------------+-------+----+\n",
      "|brand|category_code                                                                                                                                                                                                                                                                                                              |day_cos           |day_sin            |hour_cos           |hour_sin           |minute_cos        |minute_sin         |month_cos         |month_sin          |price    |product_id|rating|second_cos         |second_sin        |user_id|year|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+---------+----------+------+-------------------+------------------+-------+----+\n",
      "|27   |[-0.4971163868904114,-0.3760594427585602,0.26036107540130615,-0.29906562715768814,1.1762625724077225,-0.5227497965097427,0.8625117354094982,-0.858953133225441,1.0982406735420227,-1.598254069685936,0.34716568142175674,0.3212483897805214,0.0097588449716568,-0.3623061617836356,-0.35975897312164307,0.2760152486152947]|0.9795299412524945|0.20129852008866006|-0.9659258262890683|-0.2588190451025208|0.5877852522924729|-0.8090169943749476|0.5000000000000001|-0.8660254037844386|0.5467455|3928      |-1    |0.30901699437494745|0.9510565162951535|1      |0.5 |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+------------------+-------------------+---------+----------+------+-------------------+------------------+-------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e0c2bd1-7a3c-47b1-b776-90299a24bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=user_features + item_features + session_features, outputCol='features')\n",
    "\n",
    "df2 = vector_assembler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88a52133-7c93-4a1c-bcc1-d821f145f2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.0,3928.0,-0.4971163868904114,-0.3760594427585602,0.26036107540130615,-0.29906562715768814,1.1762625724077225,-0.5227497965097427,0.8625117354094982,-0.858953133225441,1.0982406735420227,-1.598254069685936,0.34716568142175674,0.3212483897805214,0.0097588449716568,-0.3623061617836356,-0.35975897312164307,0.2760152486152947,27.0,0.5467455,0.5,-0.8660254037844386,0.5000000000000001,0.20129852008866006,0.9795299412524945,-0.2588190451025208,-0.9659258262890683,-0.8090169943749476,0.5877852522924729,0.9510565162951535,0.30901699437494745]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('features').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9cf026b-a35e-44ea-adae-395c9e927231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import RegressionEvaluator \n",
    "# from pyspark.ml.recommendation import ALS \n",
    "\n",
    "# als = ALS(maxIter=5,\n",
    "#           userCol=\"user_id\",\n",
    "#           itemCol=\"product_id\",\n",
    "#           ratingCol=\"rating\",\n",
    "#           coldStartStrategy=\"drop\")\n",
    "\n",
    "# als_model = als.fit(train_df)\n",
    "\n",
    "# predictions = als_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a905e0df-d01a-402f-bf66-22668fbfb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "# rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "# r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "# print(\"Root-mean-square error = \" + str(rmse) + '\\n' + 'R2 = ' + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c305a19-e10c-4f6f-8d9f-aff80ea742e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# grid = ParamGridBuilder()\n",
    "# grid = grid.addGrid(als.blockSize, [1024, 2048, 4096])\\\n",
    "#            .addGrid(als.regParam, np.logspace(1e-3,1e-1))\\\n",
    "#                     .build()\n",
    "\n",
    "# evaluator1 = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# cv = CrossValidator(estimator=als,\n",
    "#                     estimatorParamMaps=grid,\n",
    "#                     evaluator=evaluator1,\n",
    "#                     parallelism=5,\n",
    "#                     numFolds=3)\n",
    "\n",
    "# cv_model_als = cv.fit(train_df)\n",
    "# model1 = cv_model_als.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d37e4692-d478-44ec-bd22-fec9f13834ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "# run(\"hdfs dfs -get project/models/model1 models/model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf76c908-ac5c-4865-ad8f-604d2c9440fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model1.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afb166-06da-47a1-9b8f-1e6e0a75b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.select(\"label\", \"prediction\")\\\n",
    "#     .coalesce(1)\\\n",
    "#     .write\\\n",
    "#     .mode(\"overwrite\")\\\n",
    "#     .format(\"csv\")\\\n",
    "#     .option(\"sep\", \",\")\\\n",
    "#     .option(\"header\",\"true\")\\\n",
    "#     .save(\"project/output/model1_predictions\")\n",
    "\n",
    "# run(\"hdfs dfs -cat project/output/model1_predictions/*.csv > output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "565dec0b-aae1-4125-8b11-8dc6a8eb37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse1 = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "# r21 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "# print(\"Root-mean-square error = \" + str(rmse1) + '\\n' + 'R2 = ' + str(r21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc50a1-bf8e-4703-86c1-bc3e5b3a69d7",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0352839b-87fa-439b-a1d8-1b1bc3a8cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_brands = 97917\n",
    "# N_products = 39699\n",
    "# N_users = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d5793fa-91bb-4c01-a2ac-8b1e2017ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Net import Content_based_filtering\n",
    "from sparktorch import serialize_torch_obj, SparkTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = Content_based_filtering(n_brands=N_brands, n_items=N_products, n_users=N_users, dim=128, brand_dim=16)\n",
    "\n",
    "torch_obj = serialize_torch_obj(\n",
    "    model=model,\n",
    "    criterion=nn.L1Loss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001\n",
    "    # model_parameters={''}\n",
    ")\n",
    "\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='features',\n",
    "    labelCol=target,\n",
    "    predictionCol='prediction',\n",
    "    torchObj=torch_obj,\n",
    "    iters=1,\n",
    "    miniBatch=16,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=user_features + item_features + session_features, outputCol='features')\n",
    "\n",
    "spark_model = Pipeline(stages=[vector_assembler, spark_model]).fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffa1180b-a730-41d3-b783-f93b0a9bd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "\n",
    "def k_fold_split(df, k=3):\n",
    "    df_with_id = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "    total_rows = df_with_id.count()\n",
    "    fold_size = total_rows // k\n",
    "    for n in range(k):\n",
    "        test_fold_start = n * fold_size\n",
    "        test_fold_end = (n + 1) * fold_size if n != k - 1 else total_rows\n",
    "\n",
    "        test_fold = df_with_id.filter((col(\"id\") >= test_fold_start) & (col(\"id\") < test_fold_end))\n",
    "\n",
    "        training_fold = df_with_id.filter((col(\"id\") < test_fold_start) | (col(\"id\") >= test_fold_end))\n",
    "\n",
    "        test_fold = test_fold.drop(\"id\")\n",
    "        training_fold = training_fold.drop(\"id\")\n",
    "\n",
    "        yield (test_fold, training_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152993d-acfd-45c1-8b37-a405323391a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparktorch import PysparkPipelineWrapper\n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "brand_dims = [8, 16]\n",
    "dims = [64, 128]\n",
    "\n",
    "evaluator2 = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "vector_assembler = VectorAssembler(inputCols=user_features + item_features + session_features, outputCol='features')\n",
    "best_model = None\n",
    "best_rmse = 100\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for test_data, train_data in k_fold_split(train_df):\n",
    "    for brand_dim in brand_dims:\n",
    "        for dim in dims:\n",
    "            model = Content_based_filtering(n_brands=N_brands, \n",
    "                                            n_items=N_products, \n",
    "                                            n_users=N_users,\n",
    "                                            brand_dim=brand_dim,\n",
    "                                            dim=dim,\n",
    "                                           )\n",
    "            torch_obj = serialize_torch_obj(\n",
    "                model=model,\n",
    "                criterion=nn.L1Loss(),\n",
    "                optimizer=torch.optim.Adam,\n",
    "                lr=0.0001,\n",
    "            )\n",
    "            spark_model = SparkTorch(\n",
    "                inputCol='features',\n",
    "                labelCol=target,\n",
    "                predictionCol='predictions',\n",
    "                torchObj=torch_obj,\n",
    "                iters=1,\n",
    "                miniBatch=16,\n",
    "                verbose=1,\n",
    "            )\n",
    "            spark_model = Pipeline(stages=[vector_assembler, spark_model]).fit(train_data)\n",
    "            trained_model = PysparkPipelineWrapper.unwrap(spark_model)\n",
    "            preds = trained_model.transform(test_data)\n",
    "            rmse = evaluator2.evaluate(preds)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = spark_model\n",
    "            iteration += 1\n",
    "            print(iteration)\n",
    "model2 = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca9104-5cb5-44a5-9b55-7bbe3bf93922",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "run(\"hdfs dfs -get project/models/model2 models/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abed47-d9bb-4d13-9455-85c87ba7b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = PysparkPipelineWrapper.unwrap(model2).transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac40159-8462-4d51-ae33-5fa4e397de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions/*.csv > output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2e101-af3a-4c27-9ca1-613f2e3462cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse2 = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "r22 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(\"Root-mean-square error = \" + str(rmse2) + '\\n' + 'R2 = ' + str(r22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07cc29-429a-40cd-8e15-7ce81652032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[str(model1),rmse1, r21], [str(model2),rmse2, r22]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02df8d41-f9b8-4034-afae-261d85e587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
